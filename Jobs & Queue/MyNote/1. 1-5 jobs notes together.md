# Laravel CSV Import with Jobs & Queue - Complete Guide

## Overview
This guide covers the complete implementation of CSV data importing into a Laravel database using background jobs and queue processing. It progresses from basic upload to advanced batch job handling.

---

## Section 1: Database Setup

### Model (PersonDB)
```php
namespace App;

use Illuminate\Database\Eloquent\Model;

class PersonDB extends Model
{
    // Define the table name for this model
    protected $table = 'person_d_b_s';
    
    // Leave empty to allow all fields to be guarded
    protected $guarded = '';
    
    // Define which columns can be mass assigned
    protected $fillable = [
        "id",           // unique identifier
        "firstname",    // person's first name
        "lastname",     // person's last name
        "email",        // person's email address
    ];
}
```

### Migration Table
```php
public function up()
{
    // Create the person_d_b_s table in database
    Schema::create('person_d_b_s', function (Blueprint $table) {
        // Auto-incrementing ID column (can be null)
        $table->bigIncrements('id')->nullable();
        
        // String column for firstname (can be null)
        $table->string("firstname")->nullable();
        
        // String column for lastname (can be null)
        $table->string("lastname")->nullable();
        
        // String column for email (can be null)
        $table->string("email")->nullable();
        
        // Automatically add created_at and updated_at timestamps
        $table->timestamps();
    });
}
```

---

## Section 2: Basic Implementation (Without Queue)

### View
```html
<!-- Form to upload CSV file -->
<form action="{{ route('mycsv') }}" method="post" enctype="multipart/form-data">
    <!-- CSRF token for security -->
    @csrf
    
    <!-- File input field for CSV upload -->
    <input type="file" name="mycsv" placeholder="submit the csv file">
    
    <!-- Submit button -->
    <input type="submit" class="btn btn-success">
</form>
```

### Route
```php
// POST route to handle CSV file upload and processing
Route::post('/mycsv', [PersonController::class, 'jobs_queue'])->name('mycsv');
```

### Controller (Basic Upload - Limited to 100 records)
```php
function jobs_queue(Request $request)
{
    // Read the CSV file and convert each row into array using str_getcsv
    // This converts CSV content into associative array format
    $data = array_map('str_getcsv', file($request->mycsv));
    
    // Get the first row (header) from the array
    // Example: ["id", "firstname", "lastname", "email"]
    $header = $data[0];
    
    // Remove the header row from data array
    // Now $data contains only the actual records (without header)
    unset($data[0]);

    // Loop through each row of data
    foreach ($data as $key => $value) {
        // Combine header names with their corresponding values
        // Example output: ["id" => "100", "firstname" => "John", "lastname" => "Doe", "email" => "john@email.com"]
        $person = array_combine($header, $value);
        
        // Insert the combined data into the database
        PersonDB::create($person);

        // Stop after inserting 100 records (limitation)
        if($key == 100) {
            break;
        }
    }

    return "DB insert the data is done";
}
```

---

## Section 3: Chunked Upload (Without Queue)

This approach splits large CSV files into smaller chunks (100-1000 records each) and processes them separately.

### Controller
```php
function jobs_queue(Request $request)
{
    // Read the entire CSV file line by line
    $data = file($request->mycsv);
    
    // Extract the header row (first line)
    $header = $data[0];
    
    // Remove header from the data array
    unset($data[0]);

    // Split the large array into smaller chunks of 100 records each
    // Example: If we have 1000 records, this creates 10 chunks of 100 records each
    $chunks = array_chunk($data, 100);

    // Define the path where temporary CSV files will be saved
    $path = resource_path('temp');  // resources/temp/

    // Check if temp directory exists, if not create it with proper permissions
    if (!file_exists($path)) {
        mkdir($path, 0755, true);  // true for recursive creation
    }

    // Loop through each chunk and save as temporary CSV file
    foreach($chunks as $key => $chunk) {
        // Create filename like: /tmp0.csv, /tmp1.csv, /tmp2.csv, etc.
        $name = "/tmp{$key}.csv";
        
        // Write chunk data to temporary CSV file
        // Path example: resources/temp/tmp0.csv
        file_put_contents($path . $name, $chunk);
    }

    return "CSV files created successfully";
}

// Function to read all temporary CSV files and store data in database
public function store()
{
    // Define path to temporary files directory
    $path = resource_path('temp');
    
    // Get all CSV files from the temp directory
    // glob() returns array of all files matching the pattern
    $files = glob("$path/*.csv");
    
    // Initialize empty array to store header
    $header = [];

    // Loop through each temporary CSV file
    foreach($files as $key => $file) {
        
        // Read CSV file and convert into array format
        $data = array_map('str_getcsv', file($file));

        // Only get header from the first file (key === 0)
        if($key === 0) {
            // Extract column names from first row
            $header = $data[0];
            
            // Remove header row from data
            unset($data[0]);
        }

        // Loop through each record in the current file
        foreach($data as $person) {
            // Combine header names with data values
            $personData = array_combine($header, $person);
            
            // Insert record into database
            PersonDB::create($personData);
        }
        
        // Delete the temporary file after processing
        // unlink($file);
    }

    return "Store Successfully";
}
```

---

## Section 4: Queue Implementation

### Prerequisites
1. Create queue table: `php artisan queue:table`
2. Run migration: `php artisan migrate`
3. Create job: `php artisan make:job PersonCsvProcess`
4. Set `.env`: `QUEUE_CONNECTION=database`
5. Start worker: `php artisan queue:work`

### Job Class
```php
namespace App\Jobs;

use App\PersonDB;
use Illuminate\Bus\Queueable;
use Illuminate\Contracts\Queue\ShouldQueue;
use Illuminate\Foundation\Bus\Dispatchable;
use Illuminate\Queue\InteractsWithQueue;
use Illuminate\Queue\SerializesModels;
use Throwable;

// This class implements ShouldQueue to make it a queued job
class PersonCsvProcess implements ShouldQueue
{
    // Use traits to enable job queueing functionality
    use Dispatchable, InteractsWithQueue, Queueable, SerializesModels;

    // Public properties to hold data passed from controller
    public $data, $header;

    // Constructor receives data and header from controller
    public function __construct($data, $header)
    {
        // Store the data passed from controller
        $this->data = $data;
        
        // Store the header (column names) passed from controller
        $this->header = $header;
    }

    // This function executes when the job is processed by the queue worker
    public function handle()
    {
        // Loop through each row in the data array
        foreach ($this->data as $person) {
            // Combine header column names with their corresponding values
            // Example: array_combine(['id', 'firstname', 'lastname', 'email'], ['1', 'John', 'Doe', 'john@email.com'])
            $personData = array_combine($this->header, $person);
            
            // Insert the combined data into the database
            PersonDB::create($personData);
        }
    }

    // This function executes if the job fails for any reason
    public function failed(Throwable $exception)
    {
        // You can send notifications, log errors, or perform cleanup here
        // Example: Send email notification about the failed job
        // Example: Log the exception for debugging
    }
}
```

### Controller with Queue
```php
function jobs_queue(Request $request)
{
    // Read the CSV file line by line from the uploaded file
    $data = file($request->mycsv);
    
    // Get the first line as header (column names)
    $header = $data[0];
    
    // Remove header from data array
    unset($data[0]);

    // Split all records into chunks of 100 records each
    // If we have 1000 records, this creates 10 chunks
    $chunks = array_chunk($data, 100);
    
    // Initialize header array (will be populated in loop)
    $header = [];

    // Loop through each chunk
    foreach ($chunks as $key => $chunk) {
        
        // Convert chunk array into CSV format array
        $data = array_map('str_getcsv', $chunk);

        // Get header only from the first chunk
        if ($key === 0) {
            // Extract column names from first chunk
            $header = $data[0];
            
            // Remove header from data
            unset($data[0]);
        }

        // Dispatch this chunk as a separate job to the queue
        // Each chunk will be processed by a queue worker asynchronously
        // Pass the data and header to the job constructor
        PersonCsvProcess::dispatch($data, $header);
    }

    return "Data queued for processing!";
}
```

---

## Section 5: Job Batching (Laravel 8+)

Job batching allows you to group multiple jobs together and track their progress collectively.

### Prerequisites
- Upgrade to Laravel 8+: `composer require laravel/framework:^8.0 --with-all-dependencies`

### Updated Job Class
```php
namespace App\Jobs;

use App\PersonDB;
use Illuminate\Bus\Batchable;           // For batch functionality
use Illuminate\Bus\Queueable;
use Illuminate\Contracts\Queue\ShouldQueue;
use Illuminate\Foundation\Bus\Dispatchable;
use Illuminate\Queue\InteractsWithQueue;
use Illuminate\Queue\SerializesModels;
use Throwable;

class PersonCsvProcess implements ShouldQueue
{
    // Add Batchable trait to enable this job to be part of a batch
    use Batchable, Dispatchable, InteractsWithQueue, Queueable, SerializesModels;

    // Properties to store data and header from controller
    public $data, $header;

    public function __construct($data, $header)
    {
        $this->data = $data;
        $this->header = $header;
    }

    // Main job execution function
    public function handle()
    {
        // Loop through each record in the data
        foreach ($this->data as $person) {
            // Match header column names with data values
            $personData = array_combine($this->header, $person);
            
            // Insert the combined record into database
            PersonDB::create($personData);
        }
    }

    // Handle job failure
    public function failed(Throwable $exception)
    {
        // Execute this code if the job fails
        // Example: Send failure notification, log error, etc.
    }
}
```

### Controller with Batching
```php
namespace App\Http\Controllers;

use App\Jobs\PersonCsvProcess;
use App\PersonDB;
use Illuminate\Http\Request;
use Illuminate\Bus\Batch;
use Illuminate\Support\Facades\Bus;
use Illuminate\Support\Facades\DB;

class PersonController extends Controller
{
    function jobs_queue(Request $request)
    {
        // Read the uploaded CSV file line by line
        $data = file($request->mycsv);
        
        // Extract header (first line containing column names)
        $header = $data[0];
        
        // Remove header from data
        unset($data[0]);

        // Split data into chunks of 100 records each
        $chunks = array_chunk($data, 100);
        
        // Initialize header variable
        $header = [];

        // Create an empty batch to group all jobs together
        // This allows tracking of all jobs as a single unit
        $batch = Bus::batch([])->dispatch();

        // Loop through each chunk of data
        foreach ($chunks as $key => $chunk) {
            
            // Convert chunk into CSV array format
            $data = array_map('str_getcsv', $chunk);

            // Extract header only from the first chunk
            if ($key === 0) {
                $header = $data[0];
                unset($data[0]);
            }

            // Add this job to the batch
            // Each chunk becomes a separate job added to the batch
            $batch->add(new PersonCsvProcess($data, $header));
        }

        // Return the batch object
        // Response contains batch ID: "9ba2444c-6239-4cac-b15a-bfe4bdcf92bf"
        return $batch;
    }

    // Get detailed information about a specific batch by its ID
    public function batch()
    {
        // Get batch ID from URL query parameter (?id=...)
        $batchId = request('id');
        
        // Find and return the batch with its current status
        // Returns progress, processed jobs, failed jobs, etc.
        return Bus::findBatch($batchId);
    }

    // Get all batches currently in progress (with real-time progress tracking)
    public function batchInProgress()
    {
        // Query database for batches that still have pending jobs
        // pending_jobs > 0 means the batch is still processing
        $batches = DB::table('job_batches')->where('pending_jobs', '>', 0)->get();

        // If there are any batches in progress, return the first one
        if(count($batches) > 0) {
            return Bus::findBatch($batches[0]->id);
        }

        // Return empty array if no batches are in progress
        return [];
    }
}
```

### Routes for Batching
```php
// Display the upload form page
Route::get('/', function () {
    return view('welcome');
});

// Handle CSV file upload and create batch of jobs
Route::post('/mycsv', [PersonController::class, 'jobs_queue'])->name('mycsv');

// View details of a specific batch by passing batch ID
// Example: /batch?id=9ba2444c-6239-4cac-b15a-bfe4bdcf92bf
Route::get('/batch', [PersonController::class, 'batch']);

// Get real-time progress of batches currently being processed
// Shows percentage completion and job status
Route::get('/batchInProgress', [PersonController::class, 'batchInProgress']);
```

### Batch Response Example
```
{
    "id": "9ba24b23-ae2e-4441-b7bb-4b320ab12230",           // Unique batch identifier
    "name": "",                                              // Batch name (optional)
    "totalJobs": 300,                                        // Total jobs in this batch
    "pendingJobs": 0,                                        // Jobs waiting to be processed
    "processedJobs": 300,                                    // Completed jobs
    "progress": 100,                                         // Completion percentage
    "failedJobs": 0,                                         // Number of failed jobs
    "options": [],                                           // Additional batch options
    "createdAt": "2024-03-23T17:04:36.000000Z",            // When batch was created
    "finishedAt": "2024-03-23T17:05:26.000000Z"            // When batch completed
}
```

---

## Key Concepts

**Jobs**: Background processes that handle time-consuming tasks asynchronously. Instead of making users wait for data to process, jobs run in the background using queue workers.

**Batch of Jobs**: Multiple jobs grouped together for collective tracking and management. Instead of tracking 300 individual jobs, you track them as one batch with overall progress.

**Queue**: A system that manages job execution in order (FIFO - First In First Out). Jobs are stored and processed one by one by queue workers.

**Chunking**: Splitting large datasets into smaller, manageable pieces for processing. Instead of processing 1000 records at once, split into 10 chunks of 100 records each.

**Fake Data Generation**: Use https://extendsclass.com/csv-generator.html to create test CSV files for development and testing.

---

## Troubleshooting

- **queue:work not working**: Run `php artisan optimize:clear` then restart the worker with `php artisan queue:work`
- **Directory not found**: Ensure the `temp` directory exists in `resources/` with proper write permissions (755)
- **Batch not showing progress**: Use the `batchInProgress` endpoint to check if batch is still being processed
- **Data not inserting**: Always remove headers before database insertion, verify column names match table structure
- **CSV parsing errors**: Check if CSV file format is correct (proper delimiters, no special characters in headers)

---

## Workflow Summary

1. **Upload**: User uploads CSV file via form
2. **Parse**: Application reads CSV and extracts header row (id, firstname, lastname, email)
3. **Chunk**: Split large dataset into chunks (100-1000 records per chunk)
4. **Queue**: Dispatch each chunk as a separate job to the queue
5. **Process**: Background worker picks up jobs and processes them asynchronously
6. **Store**: Each job inserts its chunk data into the database
7. **Track**: Batch system monitors overall progress and completion status
8. **Complete**: All jobs processed, batch marked as finished with statistics
