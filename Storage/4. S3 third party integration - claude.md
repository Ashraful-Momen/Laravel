# Laravel S3 Integration Guide for Third-Party S3 Storage

## Option 1: S3FS-FUSE Mount Solution (Recommended for Transparency)

This approach mounts your S3 bucket as a filesystem, making it completely transparent to your Laravel application.

### 1. Update Docker Configuration

Create a new Dockerfile for your app service:

```dockerfile
# docker/php/Dockerfile.s3
FROM php:8.1-fpm

# Install s3fs-fuse
RUN apt-get update && apt-get install -y \
    s3fs \
    fuse \
    && rm -rf /var/lib/apt/lists/*

# Copy PHP extensions and configurations
# ... (your existing PHP setup)

# Create mount points
RUN mkdir -p /mnt/s3-public /mnt/s3-storage

# Copy s3fs credentials and mount script
COPY docker/s3/passwd-s3fs /etc/passwd-s3fs
RUN chmod 600 /etc/passwd-s3fs

COPY docker/s3/mount-s3.sh /usr/local/bin/mount-s3.sh
RUN chmod +x /usr/local/bin/mount-s3.sh

# Set entrypoint
ENTRYPOINT ["/usr/local/bin/mount-s3.sh"]
CMD ["php-fpm"]
```

### 2. Create S3FS Credentials File

Create `docker/s3/passwd-s3fs`:

```
YOUR_ACCESS_KEY:YOUR_SECRET_KEY
```

### 3. Create Mount Script

Create `docker/s3/mount-s3.sh`:

```bash
#!/bin/bash

# Mount S3 buckets
s3fs your-bucket-name:/public /var/www/html/public/uploads \
    -o url=https://your-s3-endpoint.com \
    -o use_path_request_style \
    -o allow_other \
    -o umask=0022 \
    -o uid=www-data \
    -o gid=www-data

s3fs your-bucket-name:/storage /var/www/html/storage/app/public \
    -o url=https://your-s3-endpoint.com \
    -o use_path_request_style \
    -o allow_other \
    -o umask=0022 \
    -o uid=www-data \
    -o gid=www-data

# Execute the original command
exec "$@"
```

### 4. Update Docker Swarm Configuration

```yaml
version: '3.8'
services:
  app:
    build:
      context: .
      dockerfile: docker/php/Dockerfile.s3
    image: instasure-dockerized_app:latest-s3
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    working_dir: /var/www/html
    volumes:
      - /home/ashraful/gitlab_project/instasure-dockerized:/var/www/html
      - /home/ashraful/gitlab_project/instasure-dockerized/docker/php/local.ini:/usr/local/etc/php/conf.d/local.ini
    cap_add:
      - SYS_ADMIN
    devices:
      - /dev/fuse
    security_opt:
      - apparmor:unconfined
    networks:
      - laravel_network
    depends_on:
      - mysql
      - redis
```

## Option 2: Laravel Filesystem Configuration (Code-Based)

This approach uses Laravel's built-in S3 driver with minimal code changes.

### 1. Install Required Package

Add to your `composer.json` and run inside the container:

```bash
docker exec -it <container_id> composer require league/flysystem-aws-s3-v3 "^3.0"
```

### 2. Update .env File

Add these S3 configuration variables to your `.env`:

```env
# S3 Compatible Storage Configuration
S3_ACCESS_KEY_ID=your_access_key
S3_SECRET_ACCESS_KEY=your_secret_key
S3_DEFAULT_REGION=us-east-1
S3_BUCKET=your-bucket-name
S3_ENDPOINT=https://your-s3-endpoint.com
S3_USE_PATH_STYLE_ENDPOINT=true

# Update filesystem disk
FILESYSTEM_DISK=s3
```

### 3. Configure config/filesystems.php

```php
'disks' => [
    // ... existing disks

    's3' => [
        'driver' => 's3',
        'key' => env('S3_ACCESS_KEY_ID'),
        'secret' => env('S3_SECRET_ACCESS_KEY'),
        'region' => env('S3_DEFAULT_REGION', 'us-east-1'),
        'bucket' => env('S3_BUCKET'),
        'url' => env('S3_URL'),
        'endpoint' => env('S3_ENDPOINT'),
        'use_path_style_endpoint' => env('S3_USE_PATH_STYLE_ENDPOINT', true),
        'throw' => false,
        'visibility' => 'public',
    ],

    'public' => [
        'driver' => 's3',
        'key' => env('S3_ACCESS_KEY_ID'),
        'secret' => env('S3_SECRET_ACCESS_KEY'),
        'region' => env('S3_DEFAULT_REGION', 'us-east-1'),
        'bucket' => env('S3_BUCKET'),
        'url' => env('S3_URL'),
        'endpoint' => env('S3_ENDPOINT'),
        'use_path_style_endpoint' => env('S3_USE_PATH_STYLE_ENDPOINT', true),
        'root' => 'public',
        'visibility' => 'public',
    ],
],
```

## Option 3: Nginx Reverse Proxy for Static Files

This approach uses Nginx to proxy static file requests directly to S3.

### 1. Update Nginx Configuration

Create `docker/nginx/default-s3.conf`:

```nginx
server {
    listen 80;
    index index.php index.html;
    error_log  /var/log/nginx/error.log;
    access_log /var/log/nginx/access.log;
    root /var/www/html/public;

    # Proxy public uploads to S3
    location ~ ^/uploads/(.*)$ {
        resolver 8.8.8.8;
        proxy_pass https://your-bucket-name.your-s3-endpoint.com/public/$1;
        proxy_set_header Host your-bucket-name.your-s3-endpoint.com;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Cache settings
        proxy_cache_valid 200 302 60m;
        proxy_cache_valid 404 1m;
        
        # CORS headers if needed
        add_header 'Access-Control-Allow-Origin' '*';
        add_header 'Access-Control-Allow-Methods' 'GET, OPTIONS';
    }

    # Proxy storage files to S3
    location ~ ^/storage/(.*)$ {
        resolver 8.8.8.8;
        proxy_pass https://your-bucket-name.your-s3-endpoint.com/storage/$1;
        proxy_set_header Host your-bucket-name.your-s3-endpoint.com;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Cache settings
        proxy_cache_valid 200 302 60m;
        proxy_cache_valid 404 1m;
    }

    # PHP processing
    location ~ \.php$ {
        try_files $uri =404;
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass app:9000;
        fastcgi_index index.php;
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        fastcgi_param PATH_INFO $fastcgi_path_info;
    }

    location / {
        try_files $uri $uri/ /index.php?$query_string;
        gzip_static on;
    }
}
```

## Option 4: Rclone Mount (Alternative to S3FS)

### 1. Create Rclone Dockerfile

```dockerfile
# docker/php/Dockerfile.rclone
FROM php:8.1-fpm

# Install rclone
RUN curl https://rclone.org/install.sh | bash

# ... rest of your PHP setup

# Copy rclone config
COPY docker/rclone/rclone.conf /root/.config/rclone/rclone.conf

# Mount script
COPY docker/rclone/mount-rclone.sh /usr/local/bin/mount-rclone.sh
RUN chmod +x /usr/local/bin/mount-rclone.sh

ENTRYPOINT ["/usr/local/bin/mount-rclone.sh"]
CMD ["php-fpm"]
```

### 2. Create Rclone Configuration

Create `docker/rclone/rclone.conf`:

```ini
[s3-storage]
type = s3
provider = Other
access_key_id = YOUR_ACCESS_KEY
secret_access_key = YOUR_SECRET_KEY
endpoint = https://your-s3-endpoint.com
acl = public-read
```

### 3. Create Mount Script

Create `docker/rclone/mount-rclone.sh`:

```bash
#!/bin/bash

# Mount S3 using rclone
rclone mount s3-storage:your-bucket-name/public /var/www/html/public/uploads \
    --daemon \
    --allow-other \
    --vfs-cache-mode full \
    --buffer-size 256M \
    --dir-cache-time 72h \
    --vfs-read-chunk-size 128M \
    --vfs-read-chunk-size-limit 1G

rclone mount s3-storage:your-bucket-name/storage /var/www/html/storage/app/public \
    --daemon \
    --allow-other \
    --vfs-cache-mode full \
    --buffer-size 256M \
    --dir-cache-time 72h \
    --vfs-read-chunk-size 128M \
    --vfs-read-chunk-size-limit 1G

# Wait for mounts to be ready
sleep 5

# Execute the original command
exec "$@"
```

## Sync Existing Files to S3

If you have existing files, create a migration script:

```bash
#!/bin/bash
# sync-to-s3.sh

# Install AWS CLI or use rclone
docker exec -it <container_id> bash -c "
    # Using AWS CLI
    aws s3 sync /var/www/html/public/uploads s3://your-bucket-name/public/ \
        --endpoint-url https://your-s3-endpoint.com

    aws s3 sync /var/www/html/storage/app/public s3://your-bucket-name/storage/ \
        --endpoint-url https://your-s3-endpoint.com
"
```

## Helper: Storage URL Generation

Create a helper function in `app/Helpers/StorageHelper.php`:

```php
<?php

namespace App\Helpers;

use Illuminate\Support\Facades\Storage;

class StorageHelper
{
    public static function url($path)
    {
        if (config('filesystems.default') === 's3') {
            return Storage::disk('s3')->url($path);
        }
        
        return Storage::url($path);
    }
    
    public static function publicUrl($path)
    {
        $s3Url = env('S3_ENDPOINT') . '/' . env('S3_BUCKET');
        return $s3Url . '/public/' . ltrim($path, '/');
    }
}
```

## Testing the Integration

1. **Test file upload:**
```php
// In a test controller or tinker
Storage::disk('s3')->put('test.txt', 'Hello from S3!');
$url = Storage::disk('s3')->url('test.txt');
echo $url;
```

2. **Test public file access:**
```bash
curl http://your-domain/uploads/test-image.jpg
curl http://your-domain/storage/test-file.pdf
```

3. **Monitor logs:**
```bash
docker service logs instasure-dockerized_app
docker service logs instasure-dockerized_nginx
```

## Performance Optimization

1. **Enable CDN/CloudFront:**
```env
S3_CDN_URL=https://cdn.your-domain.com
```

2. **Configure caching headers in Nginx:**
```nginx
location ~* \.(jpg|jpeg|png|gif|ico|css|js)$ {
    expires 365d;
    add_header Cache-Control "public, immutable";
}
```

3. **Use Laravel's cache for S3 file listings:**
```php
Cache::remember('s3-files-list', 3600, function () {
    return Storage::disk('s3')->files('public');
});
```

## Troubleshooting

1. **Permission issues:**
   - Ensure S3 bucket has proper ACL settings
   - Check IAM/access key permissions

2. **Mount not working:**
   - Check if fuse is enabled: `modprobe fuse`
   - Verify credentials in passwd-s3fs

3. **Slow performance:**
   - Increase cache settings in s3fs/rclone
   - Use CDN for frequently accessed files

## Security Considerations

1. Never commit credentials to git
2. Use IAM roles with minimal permissions
3. Enable S3 bucket versioning
4. Set up proper CORS policies
5. Use signed URLs for private files

Choose the option that best fits your infrastructure and requirements. Option 1 (S3FS-FUSE) provides the most transparent integration without code changes.
