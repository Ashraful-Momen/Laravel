This requires a hybrid approach with intelligent failover and sync. Here's the complete solution:

## STEP 1: Update filesystems.php - Hybrid Configuration

Replace `config/filesystems.php`:

```php
<?php

return [
    'default' => 'local', // Keep system files local
    'cloud' => 's3',

    'disks' => [
        'local' => [
            'driver' => 'local',
            'root' => storage_path('app'),
            'throw' => false,
        ],

        'public' => [
            'driver' => 'local',
            'root' => storage_path('app/public'),
            'url' => env('APP_URL').'/storage',
            'visibility' => 'public',
            'throw' => false,
        ],

        // S3 for user uploads and images
        's3-users' => [
            'driver' => 's3',
            'key' => env('S3_KEY'),
            'secret' => env('S3_SECRET'),
            'region' => env('S3_REGION'),
            'bucket' => env('S3_BUCKET'),
            'url' => env('S3_URL'),
            'endpoint' => env('S3_ENDPOINT'),
            'use_path_style_endpoint' => env('S3_USE_PATH_STYLE', true),
            'root' => 'users',
            'visibility' => 'public',
            'throw' => false,
        ],

        // S3 for system logs
        's3-logs' => [
            'driver' => 's3',
            'key' => env('S3_KEY'),
            'secret' => env('S3_SECRET'),
            'region' => env('S3_REGION'),
            'bucket' => env('S3_BUCKET'),
            'url' => env('S3_URL'),
            'endpoint' => env('S3_ENDPOINT'),
            'use_path_style_endpoint' => env('S3_USE_PATH_STYLE', true),
            'root' => 'logs',
            'visibility' => 'private',
            'throw' => false,
        ],

        // Main S3 disk
        's3' => [
            'driver' => 's3',
            'key' => env('S3_KEY'),
            'secret' => env('S3_SECRET'),
            'region' => env('S3_REGION'),
            'bucket' => env('S3_BUCKET'),
            'url' => env('S3_URL'),
            'endpoint' => env('S3_ENDPOINT'),
            'use_path_style_endpoint' => env('S3_USE_PATH_STYLE', true),
            'visibility' => 'public',
            'throw' => false,
        ],

        // Local failover for user files
        'users-failover' => [
            'driver' => 'local',
            'root' => storage_path('app/users-failover'),
            'visibility' => 'public',
            'throw' => false,
        ],

        // Local failover for logs
        'logs-failover' => [
            'driver' => 'local',
            'root' => storage_path('logs-failover'),
            'throw' => false,
        ],
    ],

    'links' => [
        public_path('storage') => storage_path('app/public'),
    ],

    // Custom failover configuration
    'failover_config' => [
        's3-users' => 'users-failover',
        's3-logs' => 'logs-failover',
        'sync_interval' => 300, // 5 minutes
        'health_check_interval' => 60, // 1 minute
    ],
];
```

## STEP 2: Create S3 Failover Service

Create `app/Services/S3FailoverService.php`:

```php
<?php

namespace App\Services;

use Illuminate\Support\Facades\Storage;
use Illuminate\Support\Facades\File;
use Illuminate\Support\Facades\Cache;
use Illuminate\Support\Facades\Log;

class S3FailoverService
{
    private $s3Disks = ['s3-users', 's3-logs'];
    private $failoverDisks = ['users-failover', 'logs-failover'];
    
    public function isS3Available(): bool
    {
        return Cache::remember('s3_health_status', 60, function () {
            try {
                $testFile = 'health-check-' . time() . '.txt';
                $result = Storage::disk('s3')->put($testFile, 'health check');
                
                if ($result) {
                    Storage::disk('s3')->delete($testFile);
                    return true;
                }
                return false;
            } catch (\Exception $e) {
                Log::warning('S3 health check failed: ' . $e->getMessage());
                return false;
            }
        });
    }

    public function getActiveDisk(string $intendedDisk): string
    {
        $failoverMap = config('filesystems.failover_config');
        
        if (!in_array($intendedDisk, $this->s3Disks)) {
            return $intendedDisk; // Not an S3 disk, return as-is
        }

        if ($this->isS3Available()) {
            // S3 is available, check if we need to sync failover data
            $this->scheduleFailoverSync($intendedDisk);
            return $intendedDisk;
        } else {
            // S3 is down, use failover
            $failoverDisk = $failoverMap[$intendedDisk] ?? 'local';
            Log::warning("S3 unavailable, using failover disk: {$failoverDisk}");
            return $failoverDisk;
        }
    }

    public function put(string $intendedDisk, string $path, $contents): string|false
    {
        $activeDisk = $this->getActiveDisk($intendedDisk);
        
        try {
            $result = Storage::disk($activeDisk)->put($path, $contents);
            
            // Log the operation for sync purposes
            if ($activeDisk !== $intendedDisk) {
                $this->logFailoverOperation('put', $intendedDisk, $activeDisk, $path);
            }
            
            return $result;
        } catch (\Exception $e) {
            Log::error("Failed to store file on {$activeDisk}: " . $e->getMessage());
            
            // If S3 failed, try failover
            if ($activeDisk === $intendedDisk && in_array($intendedDisk, $this->s3Disks)) {
                $failoverMap = config('filesystems.failover_config');
                $failoverDisk = $failoverMap[$intendedDisk];
                
                try {
                    $result = Storage::disk($failoverDisk)->put($path, $contents);
                    $this->logFailoverOperation('put', $intendedDisk, $failoverDisk, $path);
                    Cache::forget('s3_health_status'); // Force recheck
                    return $result;
                } catch (\Exception $e2) {
                    Log::error("Failover also failed for {$failoverDisk}: " . $e2->getMessage());
                    return false;
                }
            }
            
            return false;
        }
    }

    public function get(string $intendedDisk, string $path): string|false
    {
        $activeDisk = $this->getActiveDisk($intendedDisk);
        
        try {
            return Storage::disk($activeDisk)->get($path);
        } catch (\Exception $e) {
            // If primary disk fails, try to get from failover
            if ($activeDisk === $intendedDisk && in_array($intendedDisk, $this->s3Disks)) {
                $failoverMap = config('filesystems.failover_config');
                $failoverDisk = $failoverMap[$intendedDisk];
                
                try {
                    return Storage::disk($failoverDisk)->get($path);
                } catch (\Exception $e2) {
                    Log::error("Failed to retrieve file from both primary and failover");
                    return false;
                }
            }
            
            return false;
        }
    }

    public function url(string $intendedDisk, string $path): string
    {
        $activeDisk = $this->getActiveDisk($intendedDisk);
        return Storage::disk($activeDisk)->url($path);
    }

    private function logFailoverOperation(string $operation, string $intendedDisk, string $actualDisk, string $path): void
    {
        $logEntry = [
            'operation' => $operation,
            'intended_disk' => $intendedDisk,
            'actual_disk' => $actualDisk,
            'path' => $path,
            'timestamp' => now()->toISOString(),
        ];

        // Store sync queue in local file
        $queueFile = storage_path('app/sync-queue.json');
        $queue = [];
        
        if (File::exists($queueFile)) {
            $queue = json_decode(File::get($queueFile), true) ?? [];
        }
        
        $queue[] = $logEntry;
        File::put($queueFile, json_encode($queue, JSON_PRETTY_PRINT));
    }

    private function scheduleFailoverSync(string $disk): void
    {
        $lastSync = Cache::get("last_sync_{$disk}", 0);
        $syncInterval = config('filesystems.failover_config.sync_interval', 300);
        
        if (time() - $lastSync > $syncInterval) {
            // Dispatch sync job
            \App\Jobs\SyncFailoverToS3::dispatch($disk)->onQueue('sync');
            Cache::put("last_sync_{$disk}", time(), 3600);
        }
    }
}
```

## STEP 3: Create Sync Job

Create `app/Jobs/SyncFailoverToS3.php`:

```php
<?php

namespace App\Jobs;

use Illuminate\Bus\Queueable;
use Illuminate\Contracts\Queue\ShouldQueue;
use Illuminate\Foundation\Bus\Dispatchable;
use Illuminate\Queue\InteractsWithQueue;
use Illuminate\Queue\SerializesModels;
use Illuminate\Support\Facades\Storage;
use Illuminate\Support\Facades\File;
use Illuminate\Support\Facades\Log;

class SyncFailoverToS3 implements ShouldQueue
{
    use Dispatchable, InteractsWithQueue, Queueable, SerializesModels;

    protected $disk;

    public function __construct(string $disk)
    {
        $this->disk = $disk;
    }

    public function handle(): void
    {
        $failoverMap = config('filesystems.failover_config');
        $failoverDisk = $failoverMap[$this->disk] ?? null;

        if (!$failoverDisk) {
            return;
        }

        Log::info("Starting sync from {$failoverDisk} to {$this->disk}");

        try {
            // Process sync queue
            $this->processSyncQueue();
            
            // Sync all files in failover directory
            $this->syncAllFailoverFiles($failoverDisk);
            
            Log::info("Sync completed for {$this->disk}");
        } catch (\Exception $e) {
            Log::error("Sync failed for {$this->disk}: " . $e->getMessage());
            throw $e;
        }
    }

    private function processSyncQueue(): void
    {
        $queueFile = storage_path('app/sync-queue.json');
        
        if (!File::exists($queueFile)) {
            return;
        }

        $queue = json_decode(File::get($queueFile), true) ?? [];
        $processedQueue = [];

        foreach ($queue as $item) {
            if ($item['intended_disk'] === $this->disk) {
                try {
                    $this->syncFile($item['actual_disk'], $this->disk, $item['path']);
                    Log::info("Synced queued file: {$item['path']}");
                } catch (\Exception $e) {
                    Log::error("Failed to sync queued file {$item['path']}: " . $e->getMessage());
                    $processedQueue[] = $item; // Keep for retry
                }
            } else {
                $processedQueue[] = $item; // Not for this disk
            }
        }

        File::put($queueFile, json_encode($processedQueue, JSON_PRETTY_PRINT));
    }

    private function syncAllFailoverFiles(string $failoverDisk): void
    {
        $files = Storage::disk($failoverDisk)->allFiles();

        foreach ($files as $file) {
            try {
                $this->syncFile($failoverDisk, $this->disk, $file);
            } catch (\Exception $e) {
                Log::error("Failed to sync file {$file}: " . $e->getMessage());
            }
        }
    }

    private function syncFile(string $sourceDisk, string $targetDisk, string $path): void
    {
        $content = Storage::disk($sourceDisk)->get($path);
        Storage::disk($targetDisk)->put($path, $content);
        
        // Delete from failover after successful sync
        Storage::disk($sourceDisk)->delete($path);
    }
}
```

## STEP 4: Create Storage Helper

Create `app/Helpers/SmartStorage.php`:

```php
<?php

namespace App\Helpers;

use App\Services\S3FailoverService;

class SmartStorage
{
    private static $failoverService;

    private static function getFailoverService(): S3FailoverService
    {
        if (!self::$failoverService) {
            self::$failoverService = app(S3FailoverService::class);
        }
        return self::$failoverService;
    }

    public static function putUserFile(string $path, $contents)
    {
        return self::getFailoverService()->put('s3-users', $path, $contents);
    }

    public static function getUserFile(string $path)
    {
        return self::getFailoverService()->get('s3-users', $path);
    }

    public static function getUserFileUrl(string $path): string
    {
        return self::getFailoverService()->url('s3-users', $path);
    }

    public static function putLogFile(string $path, $contents)
    {
        return self::getFailoverService()->put('s3-logs', $path, $contents);
    }

    public static function getLogFile(string $path)
    {
        return self::getFailoverService()->get('s3-logs', $path);
    }

    public static function isS3Available(): bool
    {
        return self::getFailoverService()->isS3Available();
    }
}
```

## STEP 5: Update nginx Configuration

Update your nginx `default.conf`:

```nginx
server {
    listen 80;
    server_name localhost 36.255.69.72;
    index index.php index.html;
    error_log  /var/log/nginx/error.log;
    access_log /var/log/nginx/access.log;
    root /var/www/html/public;

    # Basic Laravel configuration
    location / {
        try_files $uri $uri/ /index.php?$query_string;
    }

    # PHP-FPM configuration
    location ~ \.php$ {
        try_files $uri =404;
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass app:9000;
        fastcgi_index index.php;
        include fastcgi_params;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        fastcgi_param PATH_INFO $fastcgi_path_info;
        
        fastcgi_read_timeout 300;
        fastcgi_connect_timeout 300;
        fastcgi_send_timeout 300;
    }

    # Serve user files from S3 with local failover
    location ~* ^/users/(.+)$ {
        # Try S3 first
        proxy_pass https://s3.brilliant.com.bd/instasure/users/$1;
        proxy_ssl_verify off;
        proxy_ssl_server_name on;
        proxy_set_header Host s3.brilliant.com.bd;
        
        add_header X-Served-By "S3-Users";
        
        # Failover to local if S3 fails
        proxy_intercept_errors on;
        error_page 404 502 503 504 = @users_failover;
    }

    location @users_failover {
        try_files /storage/users-failover/$1 =404;
        add_header X-Served-By "Local-Users-Failover";
    }

    # Serve static assets from local (no S3)
    location ~* \.(jpg|jpeg|png|gif|ico|css|js|svg|woff|woff2|ttf|eot)$ {
        try_files $uri =404;
        expires 1y;
        add_header Cache-Control "public, immutable";
        add_header X-Served-By "Local-Static";
    }

    # Serve storage files from local
    location ~* ^/storage/(.+)$ {
        try_files /storage/app/public/$1 =404;
        expires 1d;
        add_header Cache-Control "public, max-age=86400";
        add_header X-Served-By "Local-Storage";
    }

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header Referrer-Policy "no-referrer-when-downgrade" always;
    add_header Content-Security-Policy "default-src 'self' http: https: data: blob: 'unsafe-inline'" always;

    server_tokens off;
    client_max_body_size 100M;

    location ~ /\. {
        deny all;
    }
}
```

## STEP 6: Create Monitoring Command

Create `app/Console/Commands/MonitorS3Health.php`:

```php
<?php

namespace App\Console\Commands;

use Illuminate\Console\Command;
use App\Services\S3FailoverService;
use App\Jobs\SyncFailoverToS3;

class MonitorS3Health extends Command
{
    protected $signature = 's3:monitor';
    protected $description = 'Monitor S3 health and trigger sync when available';

    public function handle()
    {
        $failoverService = app(S3FailoverService::class);
        
        $this->info('Checking S3 health...');
        
        if ($failoverService->isS3Available()) {
            $this->info('S3 is available - triggering sync jobs');
            
            // Dispatch sync jobs for all S3 disks
            SyncFailoverToS3::dispatch('s3-users')->onQueue('sync');
            SyncFailoverToS3::dispatch('s3-logs')->onQueue('sync');
            
            $this->info('Sync jobs dispatched');
        } else {
            $this->warn('S3 is unavailable - using failover storage');
        }
    }
}
```

## STEP 7: Usage Examples

**In your Laravel controllers:**

```php
<?php

namespace App\Http\Controllers;

use Illuminate\Http\Request;
use App\Helpers\SmartStorage;

class UserController extends Controller
{
    public function uploadImage(Request $request)
    {
        $file = $request->file('image');
        $path = 'avatars/' . auth()->id() . '/' . $file->getClientOriginalName();
        
        // This will use S3 if available, local failover if not
        $result = SmartStorage::putUserFile($path, file_get_contents($file));
        
        if ($result) {
            $url = SmartStorage::getUserFileUrl($path);
            return response()->json(['success' => true, 'url' => $url]);
        }
        
        return response()->json(['success' => false]);
    }
}
```

**For logging:**

```php
// In your log configuration or custom logger
SmartStorage::putLogFile('application/' . date('Y-m-d') . '.log', $logContent);
```

## STEP 8: Add to Scheduler

In `app/Console/Kernel.php`:

```php
protected function schedule(Schedule $schedule)
{
    // Monitor S3 health every minute
    $schedule->command('s3:monitor')->everyMinute();
}
```

## STEP 9: Deploy and Test

```bash
# Deploy the updated configuration
docker stack rm laravel
docker stack deploy -c docker-stack.yml laravel

# Test the system
docker exec -it $(docker ps -q -f name=laravel_app) bash

# Test S3 status
php artisan s3:monitor

# Test file upload
php artisan tinker
```

```php
// Test smart storage
use App\Helpers\SmartStorage;

// Upload user file (goes to S3 if available, local if not)
$result = SmartStorage::putUserFile('test/user-image.jpg', 'fake image content');
var_dump($result);

// Check S3 status
$status = SmartStorage::isS3Available();
echo "S3 Available: " . ($status ? 'Yes' : 'No') . "\n";

exit
```

This setup provides:
- User files and logs go to S3 when available
- Automatic failover to local storage when S3 is down
- Automatic sync back to S3 when connection is restored
- System files stay local for better performance
- Health monitoring and queue-based sync operations
